# Agent Server

A Spring Boot-based backend for an AI Agent, leveraging **LangChain4j** for LLM orchestration and **Spring AI** for RAG capabilities.

## ğŸš€ Key Features

- **Multi-Model Support**: Supports multiple LLM configurations (OpenAI, local models via base URL).
- **RAG (Retrieval-Augmented Generation)**: Built-in support for ingesting and searching documents to enhance AI responses.
- **MCP (Model Context Protocol)**: Integration with MCP servers for extended capabilities.
- **Streaming Responses**: Real-time token streaming using WebFlux.
- **Dynamic Configuration**: Hot-swappable LLM settings via API.

## ğŸ›  Tech Stack

- **Java**: 21
- **Framework**: Spring Boot 3.3.2
- **LLM Library**: LangChain4j 0.32.0 (OpenAI, Embeddings)
- **Reactive Stack**: Project Reactor (WebFlux)
- **Build Tool**: Maven

## ğŸ“‚ Project Structure

```text
agent-server/
â”œâ”€â”€ src/main/java/com/example/agent/
â”‚   â”œâ”€â”€ controller/      # REST Endpoints (Chat, Config, RAG)
â”‚   â”œâ”€â”€ service/         # Business Logic (ModelConfig, Rag, MCP)
â”‚   â”œâ”€â”€ model/           # Data Models
â”‚   â””â”€â”€ config/          # Application Configurations
â”œâ”€â”€ src/main/resources/
â”‚   â”œâ”€â”€ application.yml  # Main configuration
â”‚   â””â”€â”€ llm_conf.json    # Default LLM configurations
â”œâ”€â”€ pom.xml              # Dependency management
â””â”€â”€ MODEL_CONFIG.md      # Detailed model setup guide
```

## ğŸ“¡ API Reference

### Chat Endpoints
- `POST /api/chat`: Send a prompt to the enabled LLM.
- `POST /api/chat/stream`: Stream response tokens in real-time.

### Model Configuration
- `GET /api/models`: List all configured models.
- `POST /api/models`: Add a new model configuration.
- `PUT /api/models/{id}`: Update an existing model.
- `DELETE /api/models/{id}`: Remove a model.

### RAG Operations
- `POST /api/rag/ingest`: Ingest text content into the vector store.
- `GET /api/rag/search`: Search for relevant documents.

## âš™ï¸ Getting Started

### Prerequisites
- JDK 21+
- Maven 3.6+

### Build and Run
```bash
./mvnw clean install
./mvnw spring-boot:run
```
The server will start on `http://localhost:8080` by default.

## ğŸ“ Configuration

Model definitions are stored in `models.json` (or `llm_conf_working.json` for active state). You can update these via the `/api/models` endpoints to change providers, base URLs, and API keys without restarting the server.
